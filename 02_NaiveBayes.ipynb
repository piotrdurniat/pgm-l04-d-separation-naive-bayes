{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przed oddaniem zadania upewnij się, że wszystko działa poprawnie.\n",
    "**Uruchom ponownie kernel** (z paska menu: Kernel$\\rightarrow$Restart) a następnie\n",
    "**wykonaj wszystkie komórki** (z paska menu: Cell$\\rightarrow$Run All).\n",
    "\n",
    "Upewnij się, że wypełniłeś wszystkie pola `TU WPISZ KOD` lub `TU WPISZ ODPOWIEDŹ`, oraz\n",
    "że podałeś swoje imię i nazwisko poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naiwny Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Naiwnego Bayesa jest jednym z najprostszych modeli grafowych. Zakłada on, że wszystkie obserwowane zmienne $X_1, X_2, \\ldots, X_N$ są warunkowo niezależne względem zmiennej $Y$ oraz, że jedyna zależność istnieje między zmienną $Y$ a zmiennymi $\\mathbf{X}$ (zobacz rysunek). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "    \n",
    "g = nx.DiGraph()\n",
    "g.add_edges_from([(\"Y\", \"X_1\"), (\"Y\", \"X_2\"), (\"Y\", \"$X_{N-1}$\"), (\"Y\", \"$X_N$\")])\n",
    "\n",
    "nx.draw(\n",
    "    g, \n",
    "    with_labels=True, \n",
    "    node_color=[\"gray\" if \"X\" in v else \"white\" for v in g.nodes()],\n",
    "    pos={\"Y\": (0, 0), \"X_1\": (-2, -1), \"X_2\": (-1, -1), \"$X_{N-1}$\": (1, -1), \"$X_N$\": (2, -1)},\n",
    "    edgecolors=\"black\",\n",
    "    node_size=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naiwny klasyfikator Bayesa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naszym celem będzie rozwiązać zadanie klasyfikacji, gdzie klasa będzie reprezentowana przez zmienną $Y$ (zakładamy, że istnieje $K$ klas), natomiast atrybuty opisujące dane instancje to $X_1, X_2, \\ldots, X_N$ (wartości te mogą być zarówno ciągłe, jak i dyskretne). \n",
    "\n",
    "Dla konkretnej instancji opisanej $x_1, x_2, \\ldots, x_N$ poszukujemy jej rzeczywistej klasy $\\hat y$, którą uzyskujemy maksymalizując prawdopodobieństwo warunkowe klasy $y_k$ pod warunkiem danych $x_1, x_2, \\ldots, x_N$:\n",
    "\n",
    "$$\\tag{1}\\hat{y} = \\operatorname*{argmax}_{k \\in \\{1, 2, \\ldots, K\\}} \\mathbb{P}(y_k | x_1, x_2, \\ldots, x_N)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykorzystując regułę Bayesa możemy to prawdopodobieństwo rozpisać jako:\n",
    "\n",
    "$$\\mathbb{P}(y_k|x_1, x_2, \\ldots, x_N) = \\frac{\\mathbb{P}(y_k)\\mathbb{P}(x_1, x_2, \\ldots, x_N | y_k)}{\\mathbb{P}(x_1, x_2, \\ldots, x_N)}$$\n",
    "\n",
    "Licznik tego ułamka możemy zapisać jako prawdopodbieństwo łączne:\n",
    "\n",
    "$$\\tag{2}\\mathbb{P}(y_k)\\mathbb{P}(x_1, x_2, \\ldots, x_N | y_k) = \\mathbb{P}(y_k, x_1, x_2, \\ldots, x_N)$$\n",
    "\n",
    "Dodatkowo możemy pominąć mianownik i zapisać, że prawdopodobieństwo $\\text{(1)}$ jest proporcjonalne do $\\text{(2)}$:\n",
    "\n",
    "$$\\mathbb{P}(y_k|x_1, x_2, \\ldots, x_N) \\propto \\mathbb{P}(y_k, x_1, x_2, \\ldots, x_N)$$\n",
    "\n",
    "Korzystając wielokrotnie z reguły łańcuchowej możemy dokonać faktoryzacji prawdopodobieństwa łącznego:\n",
    "\n",
    "$$\\tag{3}\n",
    "\\begin{align}\n",
    "\\mathbb{P}(y_k, x_1, x_2, \\ldots, x_N) & = \\mathbb{P}(x_1, x_2, \\ldots, x_N, y_k)\\\\\n",
    "& = \\mathbb{P}(x_1 | x_2, \\ldots, x_N, y_k)\\mathbb{P}(x_2, \\ldots, x_N, y_k) \\\\\n",
    "& \\ldots \\\\\n",
    "& = \\mathbb{P}(x_1 | x_2, \\ldots, x_N, y_k)\\mathbb{P}(x_2 | \\ldots, x_N, y_k) \\ldots \\mathbb{P}(x_{N-1}|x_N, y_k) \\mathbb{P}(x_N | y_k)\\mathbb{P}(y_k)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\"Naiwność\" tego modelu zakłada, że zmienne $X_1, \\ldots, X_N$ są niezależne pod warunkiem $Y$, stąd:\n",
    "\n",
    "$$\\tag{4}\\mathbb{P}(x_i | x_{i+1}, x_{i+2}, \\ldots, x_N, y_k) = \\mathbb{P}(x_i|y_k)$$\n",
    "\n",
    "Aplikując $\\text{(4)}$ do $\\text{(3)}$ otrzymujemy:\n",
    "\n",
    "$$\\mathbb{P}(y_k|x_1, x_2, \\ldots, x_N) \\propto \\mathbb{P}(y_k, x_1, x_2, \\ldots, x_N) = \\mathbb{P}(y_k)\\mathbb{P}(x_1|y_k)\\mathbb{P}(x_2|y_k)\\ldots\\mathbb{P}(x_N|y_k) = \\mathbb{P}(y_k)\\prod_{i=1}^{N}\\mathbb{P}(x_i|y_k)$$\n",
    "\n",
    "$$\\mathbb{P}(y_k|x_1, x_2, \\ldots, x_N) \\propto \\mathbb{P}(y_k)\\prod_{i=1}^{N}\\mathbb{P}(x_i|y_k)$$\n",
    "\n",
    "Ostatecznie otrzymujemy:\n",
    "\n",
    "$$\\hat y = \\operatorname*{argmax}_{k \\in \\{1, 2, \\ldots, K\\}} \\mathbb{P}(y_k)\\prod_{i=1}^{N}\\mathbb{P}(x_i|y_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zmienne ciągłe\n",
    "Będziemy się tutaj posługiwać zbiorem danych Iris, który posiada tylko ciągłe atrybuty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets as sk_ds\n",
    "from sklearn import model_selection as sk_ms\n",
    "\n",
    "\n",
    "def load_iris_dataset():\n",
    "    X, y = sk_ds.load_iris(return_X_y=True)\n",
    "    X = pd.DataFrame(X, columns=[\n",
    "        \"sepal-length\",\n",
    "        \"sepal-width\",\n",
    "        \"petal-length\",\n",
    "        \"petal-width\",\n",
    "    ])\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te = sk_ms.train_test_split(X, y, train_size=0.8, stratify=y)\n",
    "    print(\"Full\", X.shape, y.shape)\n",
    "    print(\"Train\", X_tr.shape, y_tr.shape)\n",
    "    print(\"Test\", X_te.shape, y_te.shape)\n",
    "\n",
    "    return {\n",
    "        \"train\": {\"X\": X_tr, \"y\": y_tr},\n",
    "        \"test\": {\"X\": X_te, \"y\": y_te},\n",
    "    }\n",
    "\n",
    "\n",
    "iris = load_iris_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris[\"train\"][\"X\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris[\"train\"][\"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementacja w bibliotece `scikit-learn`\n",
    "Załóżmy, że wszystkie zmienne tutaj pochodzą z rozkładu normalnego - użyjemy klasy `GaussianNB` (model naiwnego Bayesa z rozkładami normalnymi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics as sk_mtr\n",
    "\n",
    "\n",
    "clf_scikit = GaussianNB()\n",
    "clf_scikit.fit(X=iris[\"train\"][\"X\"], y=iris[\"train\"][\"y\"])\n",
    "\n",
    "for split in (\"train\", \"test\"):\n",
    "    print(split)\n",
    "    print(sk_mtr.classification_report(\n",
    "        y_true=iris[split][\"y\"],\n",
    "        y_pred=clf_scikit.predict(X=iris[split][\"X\"]),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementacja w bibliotece `pyro`\n",
    "Poniżej zamieszczono przykładową implementację modelu naiwnego Baysa za pomocą biblioteki Pyro. Nie będziemy wchodzić w szczegóły, ale zachęcamy aby przeanalizować krok po kroku każdą z metod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.params import param_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "from pyro import distributions as dist\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "\n",
    "pyro.enable_validation(True)\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class GaussianNBClassifier:\n",
    "    def __init__(self, num_epochs=500, lr=1e-2):\n",
    "        self._num_epochs = num_epochs\n",
    "        self._lr = lr\n",
    "        \n",
    "        self._num_cls = None\n",
    "        \n",
    "        self._c_logits = None        \n",
    "        self._num_probs = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        pyro.clear_param_store()\n",
    "\n",
    "        svi = pyro.infer.SVI(\n",
    "            model=self._model,\n",
    "            guide=self._guide,\n",
    "            optim=pyro.optim.Adam({'lr': self._lr}),\n",
    "            loss=pyro.infer.Trace_ELBO(),\n",
    "        )\n",
    "\n",
    "        with tqdm(range(self._num_epochs)) as pbar:\n",
    "            for epoch in pbar:\n",
    "                loss = svi.step(X, y)\n",
    "\n",
    "                if epoch % 100 == 0:\n",
    "                    print(f\"Epoch: {epoch} Loss = {loss:.3f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred = pyro.infer.Predictive(\n",
    "            model=self._model,\n",
    "            guide=self._guide,\n",
    "            num_samples=1,\n",
    "            return_sites=('logP(c|x)',),\n",
    "        )\n",
    "        log_pcx = pred(X)['logP(c|x)'].detach().squeeze(0).squeeze(0)\n",
    "        y_pred = torch.argmax(log_pcx, dim=-1)\n",
    "        return y_pred\n",
    "    \n",
    "    def _model(self, X, y=None):    \n",
    "        if y is not None:  # training mode\n",
    "            self._num_cls = max(y) + 1\n",
    "            \n",
    "            numerical_cols = X.columns.values\n",
    "                    \n",
    "            self._init_c_logits()\n",
    "            self._init_num_params(X, numerical_cols)\n",
    "            self._observe_numerical_features_given_classes(X, y)\n",
    "        else:\n",
    "            self._get_classes_log_probs(X)\n",
    "        \n",
    "    def _guide(self, X, y=None):\n",
    "        pass  # This is meant to be an empty function\n",
    "    \n",
    "    def _init_c_logits(self):\n",
    "        self._c_probs = pyro.param(\n",
    "            'c_probs',\n",
    "            lambda: torch.ones(self._num_cls).div(self._num_cls),\n",
    "            constraint=constraints.simplex,\n",
    "        )\n",
    "        \n",
    "    def _init_num_params(self, X, numerical_cols):\n",
    "        self._num_dists = {\n",
    "            col: {\n",
    "                'mu': pyro.param(f'{col}_mu', lambda: torch.zeros(self._num_cls)),\n",
    "                'sigma': pyro.param(\n",
    "                    f'{col}_sigma',\n",
    "                    lambda: torch.ones(self._num_cls),\n",
    "                    constraint=constraints.positive,\n",
    "                ),\n",
    "            }\n",
    "            for col in numerical_cols\n",
    "        }\n",
    "        \n",
    "    def _observe_numerical_features_given_classes(self, X, y):\n",
    "        for c in range(self._num_cls):\n",
    "            x_c = X[y==c]\n",
    "            with pyro.plate(f'data-numerical-{c}', x_c.shape[0]):\n",
    "                for nc, v in self._num_dists.items():\n",
    "                    pyro.sample(\n",
    "                        f'x_{nc}|c={c}', \n",
    "                        dist.Normal(v['mu'][c], v['sigma'][c]),\n",
    "                        obs=torch.tensor(x_c[nc].values),\n",
    "                    )\n",
    "                    \n",
    "    def _get_log_likelihood(self, X):\n",
    "        log_lk = []\n",
    "        \n",
    "        for c in range(self._num_cls):\n",
    "            lps = []\n",
    "            \n",
    "            lps.extend([\n",
    "                dist.Normal(v['mu'][c], v['sigma'][c]).log_prob(torch.tensor(X[nc].values))\n",
    "                for nc, v in self._num_dists.items()\n",
    "            ])\n",
    "\n",
    "            log_lk.append(torch.stack(lps).sum(dim=0))\n",
    "            \n",
    "        return torch.stack(log_lk).t()\n",
    "    \n",
    "    def _get_classes_log_probs(self, X):\n",
    "        \n",
    "        log_lk = self._get_log_likelihood(X)\n",
    "\n",
    "        log_pcx = pyro.deterministic('logP(c|x)', self._c_probs.log() + log_lk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pyro = GaussianNBClassifier(num_epochs=1000)\n",
    "clf_pyro.fit(X=iris[\"train\"][\"X\"], y=iris[\"train\"][\"y\"])\n",
    "\n",
    "\n",
    "for split in (\"train\", \"test\"):\n",
    "    print(split)\n",
    "    print(sk_mtr.classification_report(\n",
    "        y_true=iris[split][\"y\"],\n",
    "        y_pred=clf_pyro.predict(X=iris[split][\"X\"]),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementacja w bibliotece `pgmpy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1 (0.75 pkt)\n",
    "\n",
    "Zaimplementuj funkcję `discretize_data`, która dokona dyskretyzacji (np. `KBinsDiscretizer`) zmiennych ciągłych w zadanym zbiorze danych. Zmienne kategoryczne/dyskretne nie powinny zostać zmienione. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83ca6cd2bf6ae0088a0c7a0a3cc2a742",
     "grade": true,
     "grade_id": "discretize-data",
     "locked": false,
     "points": 0.75,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def discretize_data(dataset: dict, n_bins: int) -> dict:\n",
    "    _dataset = deepcopy(dataset)\n",
    "    \n",
    "    X_train = _dataset[\"train\"][\"X\"]\n",
    "    X_test = _dataset[\"test\"][\"X\"]\n",
    "    \n",
    "    discrete_cols = X_train.select_dtypes('category').columns.values\n",
    "    continuous_cols = [c for c in X_train.columns if c not in discrete_cols]\n",
    "\n",
    "    # TU WPISZ KOD\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return _dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 2 (0.4 pkt)\n",
    "\n",
    "Zaimplementuj funkcję `build_model`, która zbuduje model Naiwnego Bayesa na podstawie obiektu `BayesianModel` (nie wykorzytuj klasy `NaiveBayes` z pgmpy!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc3c2d070fca34c96f6a8fee90a1daf8",
     "grade": true,
     "grade_id": "build-model",
     "locked": false,
     "points": 0.4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pgmpy.models import BayesianModel\n",
    "\n",
    "\n",
    "def build_model(dataset: dict) -> BayesianModel:\n",
    "    # TU WPISZ KOD\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 3 (0.5 pkt)\n",
    "\n",
    "Zaimplementuj funkcję `fit_model`, która dopasuje parametry modelu Naiwnego Bayesa. Użyj dowolnej metody estymacji (np. Maximum Likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0d6f2b245d2ef9edbc0a3dec8479578",
     "grade": true,
     "grade_id": "fit-model",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def fit_model(model: BayesianModel, training_data: dict) -> BayesianModel:\n",
    "    # TU WPISZ KOD\n",
    "    raise NotImplementedError()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 4 (0.75 pkt)\n",
    "\n",
    "Zaimplementuj funkcję `predict_pgmpy`, która zwróci predykcje modelu Naiwnego Bayesa dla zadanych danych `X`. Użyj dowolnej metody inferencji (np. Variable Elimination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf83cd2aa629bf9cff640e1a81b62d7b",
     "grade": true,
     "grade_id": "predict-pgmpy",
     "locked": false,
     "points": 0.75,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def predict_pgmpy(model, X):\n",
    "    y_pred = []\n",
    "    \n",
    "    # TU WPISZ KOD\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykorzystajmy teraz zaimplementowane funkcje, aby wyuczyć model Naiwnego Bayesa w pgmpy i sprawdźmy jakość działania modelu na zdyskretyzowanych danych. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_discrete = discretize_data(dataset=iris, n_bins=5)\n",
    "clf_pgmpy = build_model(dataset=iris_discrete)\n",
    "clf_pgmpy = fit_model(model=clf_pgmpy, training_data=iris_discrete[\"train\"])\n",
    "\n",
    "\n",
    "for split in (\"train\", \"test\"):\n",
    "    print(split)\n",
    "    print(sk_mtr.classification_report(\n",
    "        y_true=iris_discrete[split][\"y\"],\n",
    "        y_pred=predict_pgmpy(model=clf_pgmpy, X=iris_discrete[split][\"X\"]),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zmienne ciągłe i dyskretne\n",
    "Wykorzystaj zbiór CMC, aby sprawdzić wszystkie modele na zbiorze z cechami dyskretnymi i ciągłymi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cmc(N=-1):\n",
    "    # Source: https://archive.ics.uci.edu/ml/datasets/Contraceptive+Method+Choice\n",
    "    df = pd.read_csv('data/cmc.data', names=[\n",
    "        'age', # numerical\n",
    "        'w-education', # categorical\n",
    "        'h-education',  # categorical\n",
    "        'num-children',  # numerical\n",
    "        'w-religion',  # binary\n",
    "        'w-working',  # binary\n",
    "        'h-occupation',  # categorical\n",
    "        'sol-index',  # categorical\n",
    "        'media-exposure',  # binary\n",
    "        'contraceptive-method-used',  # class\n",
    "    ])\n",
    "\n",
    "    cat_cols = [\n",
    "        'w-education', # categorical\n",
    "        'h-education',  # categorical\n",
    "        'h-occupation',  # categorical\n",
    "        'sol-index',  # categorical\n",
    "    ]\n",
    "    bin_cols = [\n",
    "        'w-religion',  # binary\n",
    "        'w-working',  # binary\n",
    "        'media-exposure',  # binary\n",
    "    ]\n",
    "\n",
    "    for col in cat_cols:\n",
    "        df[col] = (df[col] - 1).astype('category')\n",
    "\n",
    "    for col in bin_cols:\n",
    "        df[col] = df[col].astype('category')    \n",
    "\n",
    "    if N != -1:\n",
    "        df = df.sample(\n",
    "            n=N,\n",
    "            weights='contraceptive-method-used',\n",
    "            random_state=2020,\n",
    "        )\n",
    "\n",
    "    X = df[df.columns[:-1]]\n",
    "    y = df['contraceptive-method-used'].values - 1\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te = sk_ms.train_test_split(X, y, train_size=0.8, stratify=y)\n",
    "    print('Full', X.shape, y.shape)\n",
    "    print('Train', X_tr.shape, y_tr.shape)\n",
    "    print('Test', X_te.shape, y_te.shape)\n",
    "\n",
    "    return {\n",
    "        'train': {'X': X_tr.reset_index(drop=True), 'y': y_tr},\n",
    "        'test': {'X': X_te.reset_index(drop=True), 'y': y_te},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmc = load_cmc(N=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmc[\"train\"][\"X\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmc[\"train\"][\"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementacja w `pyro`\n",
    "Poprzednio użyta implementacja Naiwnego Bayesa w bibliotece Pyro nie obsługuje zmiennych dyskretnych. Poniżej zamieszczamy implementację obsługująca oba typy zmiennych. Zachęcamy do dokładniejszej analizy kodu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullNBClassifier(GaussianNBClassifier):\n",
    "    def __init__(self, num_epochs=500, lr=1e-2):\n",
    "        super().__init__(num_epochs, lr)\n",
    "        self._cat_probs = None\n",
    "        \n",
    "    # fit() from base class\n",
    "    \n",
    "    # predict() from base class\n",
    "    \n",
    "    def _model(self, X, y=None):  # Override  \n",
    "        if y is not None:  # training mode\n",
    "            self._num_cls = max(y) + 1\n",
    "            \n",
    "            categorical_cols = X.select_dtypes('category').columns.values  # Changed\n",
    "            numerical_cols = [c for c in X.columns if c not in categorical_cols]  # Changed\n",
    "                    \n",
    "            self._init_c_logits()\n",
    "            self._init_num_params(X, numerical_cols)\n",
    "            self._init_cat_params(X, categorical_cols)  # Added \n",
    "            \n",
    "            self._observe_numerical_features_given_classes(X, y)\n",
    "            self._observe_categorical_features_given_classes(X, y)  # Added\n",
    "        else:\n",
    "            self._get_classes_log_probs(X)\n",
    "        \n",
    "    # _guide() from base class\n",
    "    \n",
    "    # _init_c_logits() from base class\n",
    "        \n",
    "    # _init_num_params() from base class\n",
    "        \n",
    "    def _init_cat_params(self, X, categorical_cols):  # Add\n",
    "        self._cat_logits = {\n",
    "            col: pyro.param(\n",
    "                f'{col}_logits', \n",
    "                lambda: torch.ones([self._num_cls, len(X[col].cat.categories)]),\n",
    "            )\n",
    "            for col in categorical_cols\n",
    "        }\n",
    "        \n",
    "    # _observe_numerical_features_given_classes from base class\n",
    "    \n",
    "    def _observe_categorical_features_given_classes(self, X, y):  # Add\n",
    "        for c in range(self._num_cls):\n",
    "            x_c = X[y==c]\n",
    "            with pyro.plate(f'data-categorical-{c}', x_c.shape[0]):\n",
    "                for cc, v in self._cat_logits.items():\n",
    "                    pyro.sample(\n",
    "                        f'x_{cc}|c={c}',\n",
    "                        dist.Categorical(logits=v[c]),\n",
    "                        obs=torch.tensor(x_c[cc].values)\n",
    "                    )\n",
    "                    \n",
    "    def _get_log_likelihood(self, X):  # Override\n",
    "        log_lk = []\n",
    "\n",
    "        for c in range(self._num_cls):\n",
    "            lps = []\n",
    "\n",
    "            lps.extend([\n",
    "                dist.Normal(v['mu'][c], v['sigma'][c]).log_prob(torch.tensor(X[nc].values))\n",
    "                for nc, v in self._num_dists.items()\n",
    "            ])\n",
    "            \n",
    "            # Added\n",
    "            lps.extend([\n",
    "                dist.Categorical(logits=v[c]).log_prob(torch.tensor(X[cc].values))\n",
    "                for cc, v in self._cat_logits.items()\n",
    "            ])\n",
    "            # End Added\n",
    "\n",
    "            log_lk.append(torch.stack(lps).sum(dim=0))\n",
    "\n",
    "        return torch.stack(log_lk).t()\n",
    "    \n",
    "    # _get_classes_log_probs() from base class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 5 (0.2 + 0.2 + 0.2 pkt)\n",
    "\n",
    "Porównaj jakość działania różnych implementacji Naiwnego Bayesa (`scikit`, `pyro` oraz `pgmpy`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "019d0ab2583cabc885267a475ebde88e",
     "grade": true,
     "grade_id": "cmc-scikit",
     "locked": false,
     "points": 0.2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Scikit \n",
    "\n",
    "# TU WPISZ KOD\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3b41d15e0077afcae143499186f7d675",
     "grade": true,
     "grade_id": "cmc-pyro",
     "locked": false,
     "points": 0.2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Pyro\n",
    "# TU WPISZ KOD\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd4fdb119913e9e8e594aa5c45300144",
     "grade": true,
     "grade_id": "cmc-pgmpy",
     "locked": false,
     "points": 0.2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Pgmpy\n",
    "# TU WPISZ KOD\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
